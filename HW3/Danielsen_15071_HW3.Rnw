\documentclass{article}
\usepackage[legalpaper, margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{cleveref}
\begin{document}

<<echo=F>>=
library("knitr")
library(ggplot2)
library(lattice)
library(caret) # for randomly splitting training/test 
library(rpart) # for building CART model
library(rpart.plot) # a library for an alternative way of plotting CART trees.
library(caTools)
library(comprehenr)

opts_chunk$set(echo = F,
               message = F,
               warning = F,
               results = F,
               fig=TRUE)

@

<<>>=
library(caret)
library(ROCR)
@

\section*{Problem 1}

\subsection*{a}
From common sense, we should observe a positive correlation between sale price and construction year, and between sale price and size of the house/lot. From \Cref{fig:price_vs_area} and \Cref{fig:price_vs_year} one can see indications of this correlation, but these patterns are too noisy and random to be of value on their own. Using predictive analytics we can obtain a much better understanding of what really affects housing prices, and create a model to predict the value of new homes.

<<>>=
ames = read.csv("ames.csv")
ames$SalePrice = as.numeric(ames$SalePrice)
@

\begin{figure}[!hb]
<<fig.height=3, echo=T>>=
ggplot(data=ames[ames$LotArea < 100000,]) +
geom_point(aes(x=LotArea,y=SalePrice))
@
\caption{Price vs Lot Area (some outliers removed)}
\label{fig:price_vs_area}
\end{figure}

\begin{figure}[!hb]
<<fig.height=3>>=
ggplot(data=ames) +
geom_point(aes(x=YearBuilt,y=SalePrice))
@
\caption{Price vs Construction year}
\label{fig:price_vs_year}
\end{figure}

\subsection*{b}
<<>>=
set.seed(657)
split = createDataPartition(ames$SalePrice, p = 0.65, list = FALSE)
ames.train = ames[split,]
ames.test = ames[-split,]
@

The root node of the three in \cref{fig:price_tree_b} is exterior material quality, so this is the most important variables in the dataset, according to the model. Further down the tree it queries ground living area and neighorhood on both sides, so these seem fairly important aswell. 
<<echo=T>>=
amesTree = rpart(SalePrice ~ ., data=ames.train)
@

\begin{figure}
<<fig.height = 5>>=
prp(amesTree)
@
\caption{Regression tree generated by the training data}
\label{fig:price_tree_b}
\end{figure}

\subsection*{c}
Since we do not have the air condition predictor in our tree, we can not answer that question. The only thing we can tell her is that central air condition is less important that these other factors, which really does not help our friend at all. This illustrates some of the limitations with the CART approach, we can not really infer anything about things which is not in the tree.

\subsection*{d}

<<echo = T, results=T>>=
cpMax <- 0.0005
cpMin <- 0.000
cpStep <- (cpMax - cpMin)/100
refit = TRUE
if (refit){
  set.seed(123) # Reproduce results in case only this chunk is executed
  cv.amestree = train(SalePrice ~.,
                 data = ames.train,
                 method = "rpart",
                 trControl = trainControl(method="cv"),
                 metric = "Rsquared",
                 tuneGrid = data.frame(.cp=seq(cpMin, cpMax, by = cpStep)))
}

cv.amestree$bestTune
@

<<echo=T>>=
cvmodel = rpart(SalePrice ~., data=ames.train, cp=cv.amestree$bestTune$cp)
@
\begin{figure}
<<echo=F>>=
prp(cv.amestree$finalModel)
@
\caption{The best tree as chosen by the cross validation}
\label{fig:cv_tree}
\end{figure}

So the best CP value is $0.00033$, and the resulting and much bigger tree can be seen in \cref{fig:cv_tree}. The root node is once again exterior material quality. In the first few layers down the tree, the size of the ground floor, basement, garage and the construction year are all prominent, supporting the initial theory that they are important.


\subsection*{e}

<<>>=
modelScore = function(preds){
  
  mean = mean(ames.train$SalePrice)
  N = nrow(ames.train)
  
  TSS = sum((ames.train$SalePrice - mean)**2)
  RSS = sum((preds - ames.train$SalePrice)**2)
  OSR2 = 1 - RSS/TSS
  print(OSR2)
  
  MAE = sum(abs(ames.train$SalePrice - preds))/N
  print(MAE)
  
  RMSE = sqrt(RSS/N)  
  print(RMSE)
}
modelScore(predict(cvmodel, newdata=ames.train, type="vector"))
modelScore(predict(amesTree, newdata=ames.train, type="vector"))
@


So the extended model greatly outperforms the default model with out of sample $R^2$ of  $0.912$ vs $0.755$, MAE of $14674$ vs $25299$ and RMSE of $21092$ vs $35198$. So we gain a lot of predictive power and accuracy, but we lose the simplicity, speed and interpretability of the first model. Also it was computationally much more expensive to find the extended model because of the cross validation.
\newpage
\section*{Problem 2}
\subsection*{a}

A reasonable proxy for the event of unplanned readmission might be the number of emergency visits, as this is also an unplanned event. From \cref{fig:emergency} we can see a small increase in the conditional probability, but we have very little data for the higher number of visits, so we should be careful to generalize too much.

Another possible indicator could be the number of lab procedures, as a patient with more procedures is perhaps likely to require more medical assistance, both planned and unplanned. From \cref{fig:procedures} however, this predictor does not seem very important. Again, we should be careful to extrapolate from the points with high number of procedures due to low amount of data points. With a more systematic analytical model, we can predict patients with high risk of readmission and use targeted intervention to reduce readmission in these groups.

<<>>=
readmission = read.csv("readmission.csv")
@

\begin{figure}
<<fig.height=2>>=
ggplot(data=readmission[readmission$numberEmergency < 25,]) +
geom_histogram(aes(x=numberEmergency, fill=(readmission==1)),position='fill', binwidth= 1) +
labs(fill="Readmission")
@
\caption{Conditional probability of readmission given the number of emergency visits (some outliers removed)}
\label{fig:emergency}
\end{figure}

\begin{figure}
<<fig.height=2>>=
ggplot(data=readmission) +
geom_histogram(aes(x=numLabProcedures, fill=(readmission==1)),position='fill') +
labs(fill="Readmission")
@
\caption{Conditional probability of readmission given the number of lap procedures}
\label{fig:procedures}
\end{figure}

<<>>=
set.seed(144)
split = createDataPartition(readmission$readmission, p = 0.75, list = FALSE)
readm.train <- readmission[split,]
readm.test <- readmission[-split,]
@

\subsection*{b}
For the true positives, we have the cost of the intervention, and in 75\% of the cases, the readmission: $\$1200 + 0.75 \cdot \$35000 = \$27450$. For the true negatives, we pay nothing. For the false positives we only pay $\$1200$ for the intervention. For the false negatives we pay $\$35000$ for the readmission. So the added cost of missclassification, ie the loss matrix is 

\begin{equation}
\begin{bmatrix}
  0 & 1200 \\
  35000 - 27450 & 0\\
\end{bmatrix}
  =
\begin{bmatrix}
  0 & 1200 \\
  7550 & 0\\
\end{bmatrix}
\end{equation}

\subsection*{c}

<<echo=T>>=
lossMatrix = cbind(c(0,7550), c(1200,0))
readmTree = rpart(readmission ~.,
                  data = readm.train,
                  method = "class",
                  parms=list(loss=lossMatrix),
                  cp = 0.001)
@
                  
\begin{figure}[!p]
<<echo=F, fig.height=4>>=
prp(readmTree, digits=3, varlen = 0, faclen = 0)
@
\caption{Decision tree for telehealth intervention}
\label{fig:tele_tree}
\end{figure}


In \cref{fig:tele_tree} we can se how the tree chose patients for intervention. If you have been admitted more than twice, you get chosen. Alo in some cases if you have only been admitted once, given some other factors. Like if you have been to the emergency room more than once, which supports the initial theory. We also find the number of lab procedures in the tree, altough near the bottom, which I initially though would not be too relevant. 

\subsection*{d}

<<echo=T, results=T>>=
preds = predict(readmTree, newdata=readm.test, type="class")
CM = table(readm.test$readmission, preds)
CM

acc = sum(diag(CM))/sum(CM)
acc
TPR <- CM[2,2]/sum(CM[2,])
TPR
FPR <- CM[1,2]/sum(CM[1,])
FPR

modelCostMatrix = cbind(c(0, 35000), c(1200, 27450))
modelCosts = sum(CM*modelCostMatrix)
modelCosts

baselineCosts = 35000*sum(CM[2,])
baselineCosts - modelCosts

@

The models accuracy is way lower than simply guessing and choosing no intervention every time, and the true positive rate is only $0.344$. However since type II errors is more than 6 times as expensive as type I errors, the resulting costs are lower than just guessing.

Using the CART model for targeted telehealth intervention reduces expected costs by \$2,267,550. Of the $2741$ pasients who got readmitted in the test dataset, the model correctly identifies $945$ of them. This saves $\$7550 \cdot 945 =$\$7,134,750 in costs, but we are also intervening "needlessly" with $4056$ pasients who would not come back anyway, increasing costs by \$4,867,200. One could argue that the intervention actually has some value for all the pasients, meaning the actual value of using the model is higher than the number above.

\subsection*{e}
<<>>=
getProfit <- function(interventionCost, percentage){
  tpCost = 35000*(1-percentage)+interventionCost
  costMatrix = cbind(c(0, 35000), c(interventionCost, tpCost))
  return(baselineCosts - sum(costMatrix*CM))
  
}
@

<<echo=T>>=
per = seq(0, 1, 0.01)
prices = seq(500, 2000, 10)

percentageDF = data.frame(
  profit = sapply(per, function(x) getProfit(1200, x)),
  percentageReduction = per
)

interventionDF = data.frame(
  profit = sapply(prices, function(x) getProfit(x, 0.25)),
  interventionPrice = prices
)
@



% Percentage: 0.19
% Cost 1650

Let $p$ be the percentage reduction, $c$ be the cost of the intervention and f the resulting profit from using the prediction model as compared to the baseline model. Then the product of the confusion matrix and the cost matrix is

\begin{equation}
\begin{split}
f(p, c) &= 2741 \cdot 35000 - (4056c + 1796 \cdot 35000 + 945(c + 35000(1-p))) \\
  &= 70000 + 33075000p - 5001c
\end{split}
\end{equation}

Fixing $p=0.25$ gives the plot in \cref{fig:price_inter}, and the intervention is cost-effective for $c < \$1667$. In the same way, fixing $c=1200$ gives \cref{fig:price_per} and the intervention is cost effective for $p > 0.179$.

\begin{figure}
<<fig.height=3>>=
ggplot(data = percentageDF) +
geom_line(aes(x=percentageReduction, y=profit)) +
geom_abline(slope=0,intercept=0, color="red")
@
\caption{Profit of using the predictive model as a function of the percentage effect of the telehealth intervention}
\label{fig:price_per}
\end{figure}

\begin{figure}
<<fig.height=3>>=
ggplot(data = interventionDF) +
geom_line(aes(x=interventionPrice, y=profit)) +
geom_abline(slope=0,intercept=0, color="red")
@
\caption{Profit of using the predictive model as a function of the price of the telehealth intervention}
\label{fig:price_inter}
\end{figure}

\subsection*{f}

The idea is too choose the ratio in the loss matrix to balance the decision rule.  By trial and error I found these values which give an intervention percentage of $4.14$ for the training set. 

<<echo =T>>=
p = 0.2
c = 1600
falsePosCost = c
falseNegCost = 35000 - (c + 35000*(1-p))

@

<<>>=

lossMatrix.m = lossMatrix.m = cbind(c(0,falseNegCost), c(falsePosCost,0))
readmTree.m = rpart(readmission ~.,
                  data = readm.train,
                  method = "class",
                  parms=list(loss=lossMatrix.m),
                  cp = 0.001)

trainPreds.m = predict(readmTree.m, newdata=readm.train, type="class")
trainCM.m = table(readm.train$readmission, trainPreds.m)
sum(trainCM.m[,2])/sum(trainCM.m)
@

The resuling confusion matrix on the test set is

<<echo=T, results=T>>=
testPreds.m = predict(readmTree.m, newdata=readm.test, type="class")
testCM.m = table(readm.test$readmission, testPreds.m)
testCM.m
sum(testCM.m[,2])/sum(testCM.m)
testCM.m[2,2]*7550
baselineCosts - sum(testCM.m*modelCostMatrix)
modelCosts - sum(testCM.m*modelCostMatrix)
@

With current budgets, applying the intervention to $1089$ ($4.28\%$) pasients identified by the model in the test set has a value of \$2,181,950, and gives a net profit of \$1,221,950 compared to the baseline. This is lower then our original model, and we therefore have an opportunity cost of (minimum) \$1,045,600 as compared to the non constrained case. Based on these findings we should increase budgets for telehealth interventions, as the potential cost reduction outweights the budget increase.





\end{document}